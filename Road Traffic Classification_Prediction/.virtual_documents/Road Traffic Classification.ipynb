














#EDA
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from summarytools import dfSummary
from phik.report import plot_correlation_matrix
from skimpy import skim
from scipy.stats import chi2_contingency

import warnings
warnings.filterwarnings('ignore')

# feature engineering libraries
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler , LabelEncoder, OrdinalEncoder , OneHotEncoder, FunctionTransformer
from sklearn.compose import ColumnTransformer
from sklearn.feature_selection import f_classif, mutual_info_classif, SelectKBest, chi2 , SequentialFeatureSelector
from sklearn.model_selection import train_test_split
from sklearn.utils import resample

# model preparation libraries
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV, SGDClassifier
from sklearn.svm import SVC , NuSVC, LinearSVC
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier , XGBRFClassifier

# model evaluation libraries
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import RepeatedKFold, StratifiedKFold

# model hyperparameter tuning
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV











# Reset display options (optional)
pd.reset_option('display.max_rows')
pd.reset_option('display.max_columns')


dataset = pd.read_csv("RTA Dataset.csv")
dataset.head(2)


# Removing white spaces and converting Time in hours and time formate
dateset = dataset.map(lambda x: x.strip() if isinstance(x, str) else x)
# dataset["Time"] = pd.to_timedelta(dataset["Time"])
dataset["Hour_of_day"] = pd.to_timedelta(dataset["Time"]).dt.components['hours']


skim(dataset)





round((dataset.isnull().sum().sum()) / (dataset.shape[0] * dataset.shape[1]) *100,2).astype(str) + " %"





def get_missing_value_info(df):
  """
  This function calculates the count and percentage of missing values for each feature in a DataFrame.

  Args:
      df: A pandas DataFrame.

  Returns:
      A DataFrame with three columns: 'count', 'percentage', and 'value_count'.
  """
  missing_counts = df.isnull().sum()
  missing_value_percent = round((missing_counts / df.shape[0]) * 100, 2)
  # value_counts = df.apply(pd.Series.value_counts, raw=False).fillna(0)
  result_df = pd.DataFrame({
      'count': missing_counts,
      'percentage': missing_value_percent# astype(str) + " %",
      # 'value_count': value_counts.iloc[0]
  })
  return result_df[result_df['count'] > 0]


# Check for missing values in all features
get_missing_value_info(dataset)





missing_value_column = get_missing_value_info(dataset)[get_missing_value_info(dataset)['percentage']>20].index
# missing_value_column = missing_value_percent[missing_value_percent > 20].index
missing_value_column


# Drop features with missing values > 20%
dataset.drop(columns=missing_value_column ,inplace=True)


categorical_features = [feature for feature in dataset.columns if dataset[feature].dtypes == 'object']
num_features=[col for col in dataset.columns if pd.api.types.is_numeric_dtype(dataset[col])]
discrete_features =[feature for feature in num_features if len(dataset[feature].unique()) < 20]
continous_features = [feature for feature in num_features if feature not in discrete_features]
print(f"Categorical features count: {len(categorical_features)}")
print(f"Numerical features count: {len(num_features)}")
print(f"Discrete features count: {len(discrete_features)}")
print(f"Contineous features count: {len(continous_features)}")


# Visualize missing values
plt.figure(figsize=(10, 6))
sns.heatmap(dataset.isnull(), cmap='viridis', cbar=True)
plt.title('Missing Values Heatmap')
plt.show()


get_missing_value_info(dataset)


# Handling missing, na, and NaN values with mode value
def handle_missing_values_with_mode(df, columns):
    """
    Imputes missing values ("na" and NaN) in specified columns of a pandas DataFrame
    with the mode, addressing potential data type issues and providing informative messages.

    Args:
        df (pd.DataFrame): The pandas DataFrame containing the data.
        columns (list): A list of column names to handle missing values in.

    Returns:
        pd.DataFrame: The DataFrame with missing values replaced with the mode or appropriate messages.

    Raises:
        ValueError: If a column is not found or the data type is not compatible with the mode calculation.
    """
    for col in columns:
        try:
            # Check for string data type
            if pd.api.types.is_string_dtype(df[col]):
                # Count occurrences of each unique value
                value_counts = df[col].value_counts()
                # Get the most frequent value (mode)
                mode_value = value_counts.idxmax()
                # Replace "na" and "NaN" with the mode
                df[col] = df[col].replace(["na", np.nan], mode_value)
            else:
                mode_value = df[col].mode()[0]

            df[col].fillna(mode_value, inplace=True)

        except KeyError:
            print(f"Column '{col}' not found in the DataFrame.")
        except ValueError as e:
            print(f"Error handling missing values in column '{col}': {e}")
    return df



def replace_multiple_values(data, replacements):
  """Replaces multiple values with specified replacements in a DataFrame or Series.

  Args:
    data: The DataFrame or Series to replace values in.
    replacements: A dictionary where keys are values to replace and values are their replacements.

  Returns:
    The DataFrame or Series with values replaced.
  """

  if isinstance(data, pd.Series):
    return data.replace(replacements, inplace=False)  # Avoid modifying original Series
  else:
    def replace_values_in_column(column):
      return column.replace(replacements, inplace=False)  # Avoid modifying original DataFrame

    return data.apply(replace_values_in_column)





# missing_value_columns = get_missing_value_info(dataset)[get_missing_value_info(dataset)['percentage']<20].index
# df = handle_missing_values_with_mode(dataset.copy(), missing_value_columns)





# Replacing null valaues with "Unknown"
def replace_null(value):
  return 'Unknown' if pd.isna(value) else value
df = dataset.applymap(replace_null)


# # Replacing 'na' Values with 'Unknown' as well.
df = replace_multiple_values(df, replacements={'na':'Unknown', 'unknown':'Unknown'})


# Visualize missing values
plt.figure(figsize=(10, 6))
sns.heatmap(df.isnull(), cmap='viridis', cbar=True)
plt.title('Missing Values Heatmap')
plt.show()


# Drop missing values rows
print(df.shape)
df.dropna(inplace=True)
print(df.shape)


missing_rows = df.isnull().sum()
missing_rows[missing_rows>0]


# Drop Duplicate rows
print(df.shape)
df.drop_duplicates()
print(df.shape)


def plot_numerical_distribution(feature, hue ="Accident_severity", data=df ,ticks_rotation = 0 , figsize=(5,3)):
    """
    Creates a visualization and calculates descriptive statistics for a numerical feature,
    colored by a categorical variable (hue).
    
    Args:
    feature (str): Name of the numerical feature to plot.
    hue (str): Name of the categorical variable for coloring.
    data (pd.DataFrame): DataFrame containing the data.
    """  
    # Descriptive statistics
    descriptive_stats = data.groupby(hue)[feature].describe(percentiles=[0.25, 0.5, 0.75])
    print(f"Descriptive Statistics for {feature} by {hue}:\n{descriptive_stats}")
    
    # Create histogram with hue
    plt.subplots(figsize=figsize)
    sns.histplot(data=data, x=feature, hue=hue, kde=True)
    plt.xlabel(feature)
    plt.ylabel("Density")
    plt.title(f"Distribution of {feature} by {hue}")
    plt.xticks(rotation = ticks_rotation)
    plt.show()


def plot_category_distribution(feature, hue = "Accident_severity", data=df, ticks_rotation = 0, figsize=(8,5)):
    """
    Creates a custom bar plot using matplotlib to visualize the distribution of categories
    in a feature, colored by a target variable (hue), with categories on the y-axis.

    Args:
      feature (str): Name of the feature to plot.
      hue (str): Name of the target variable for coloring.
      data (pd.DataFrame): DataFrame containing the data.
    """
    # Get the number of occurrences for each category
    n_category = dataset[feature].value_counts()

    # Create a colormap (adjust the colormap name as desired)
    import matplotlib.cm as cm
    cmap = cm.tab10  # Choose a colormap from matplotlib.cm

    # Create the bar chart
    fig, ax = plt.subplots(figsize=figsize)
    plt.barh(n_category.index, n_category.values, color=cmap(range(len(n_category))))  
    plt.xlabel("Number of occurrences")
    plt.ylabel("Category")
    plt.title(f"Distribution of category [ {feature} ]")
    plt.xticks(rotation = ticks_rotation)
    plt.gca().invert_yaxis()  # Invert y-axis for horizontal display
    plt.show()

def plot_category_counts(feature, hue = "Accident_severity", data=df, ticks_rotation = 0 , figsize=(5,3), counts = False):
    """
    Creates a count plot using seaborn to visualize the distribution of categories in a feature,
    colored by a target variable (hue).

    Args:
      feature (str): Name of the feature to plot.
      hue (str): Name of the target variable for coloring.
      data (pd.DataFrame): DataFrame containing the data.
    """
    if counts == True:
        display(data[feature].value_counts())
    else:
        print("Note: Please pass count paramaeter as 'counts = True' if you want count deatil.")
    fig, ax = plt.subplots(figsize=figsize)
    sns.countplot(x=feature, hue=hue, data=data)
    plt.xlabel(feature)
    plt.xticks(rotation = ticks_rotation)  
    plt.ylabel("Count")
    plt.title(f"Counts of Categories in [ {feature} ] by {hue}")
    plt.show()


num_features = list(df.select_dtypes(include=['int64','float64']).columns)
num_features
# Set the figure size
plt.figure(figsize=(15, 4))  # Adjust as needed
# Calculate the number of rows and columns for the subplots
num_rows = (len(num_features) + 2) // 3  # Adjust the number of columns as needed
num_cols = 3
# Create subplots with KDE plots
for i, num in enumerate(num_features):
    plt.subplot(num_rows, num_cols, i + 1)
    # sns.kdeplot(df_original[num], fill=True, color='skyblue', linewidth=2)
    sns.kdeplot(df[num], fill=True, color='green', linewidth=2)
    plt.title(f'Distribution of {num}')
    plt.xlabel(num)
    plt.ylabel('Density')

# plt.tight_layout()
plt.show()


plot_numerical_distribution(feature="Time" , ticks_rotation=0 , figsize=(12,4))


plot_numerical_distribution(feature="Hour_of_day" , ticks_rotation=0 , figsize=(12,4))


# plot_numerical_distribution(feature="Number_of_vehicles_involved" , ticks_rotation=0 , figsize=(10,4))
plot_category_distribution(feature="Number_of_vehicles_involved" , figsize=(10,3))
plot_category_counts(feature="Number_of_vehicles_involved", ticks_rotation=0 , figsize=(10,4))


# plot_numerical_distribution(feature="Number_of_casualties" , ticks_rotation=0 , figsize=(10,4))
plot_category_distribution(feature="Number_of_casualties" , figsize=(10,3))
plot_category_counts(feature="Number_of_casualties", ticks_rotation=0 , figsize=(10,4))


print(categorical_features)
# df.columns


df["Age_band_of_driver"] = df["Age_band_of_driver"].replace("Unknown", "18-30")
plot_category_distribution(feature="Age_band_of_driver" , figsize=(10,3))
plot_category_counts(feature="Age_band_of_driver", ticks_rotation=0 , figsize=(10,4))


df["Sex_of_driver"] = df["Sex_of_driver"].replace("Unknown", "Male")
plot_category_distribution(feature="Sex_of_driver" , figsize=(10,2))
plot_category_counts(feature="Sex_of_driver", ticks_rotation=0 , figsize=(10,4), counts=True)


plot_category_distribution(feature="Educational_level" , figsize=(10,3))
plot_category_counts(feature="Educational_level", ticks_rotation=0 , figsize=(12,3), counts=False)


plot_category_distribution(feature="Vehicle_driver_relation" , figsize=(8,2))
plot_category_counts(feature="Vehicle_driver_relation", ticks_rotation=0 , figsize=(8,3), counts=False)


plot_category_distribution(feature="Driving_experience" , figsize=(10,3))
plot_category_counts(feature="Driving_experience", ticks_rotation=0 , figsize=(10,3), counts=False)


plot_category_distribution(feature="Type_of_vehicle" , figsize=(10,4))
plot_category_counts(feature="Type_of_vehicle", ticks_rotation=90 , figsize=(10,3))


plot_category_distribution(feature="Owner_of_vehicle" , figsize=(10,2))
plot_category_counts(feature="Owner_of_vehicle", ticks_rotation=0 , figsize=(8,3),counts=False)


plot_category_distribution(feature="Area_accident_occured" , figsize=(8,3))
plot_category_counts(feature="Area_accident_occured", ticks_rotation=90 , figsize=(8,3))


plot_category_distribution(feature="Lanes_or_Medians" , figsize=(8,3))
plot_category_counts(feature="Lanes_or_Medians", ticks_rotation=90 , figsize=(8,5))


plot_category_distribution(feature="Road_allignment" , figsize=(8,3))
plot_category_counts(feature="Road_allignment", ticks_rotation=90 , figsize=(8,5))


plot_category_distribution(feature="Types_of_Junction" , figsize=(8,2))
plot_category_counts(feature="Types_of_Junction", ticks_rotation=0 , figsize=(10,4))


print(categorical_features)


plot_category_distribution(feature="Road_surface_type" , figsize=(8,2))
plot_category_counts(feature="Road_surface_type", ticks_rotation=0 , figsize=(12,3),counts=False)


plot_category_distribution(feature="Road_surface_conditions" , figsize=(8,2))
plot_category_counts(feature="Road_surface_conditions", ticks_rotation=0 , figsize=(10,3),counts=False)


plot_category_distribution(feature="Light_conditions" , figsize=(8,2))
plot_category_counts(feature="Light_conditions", ticks_rotation=0 , figsize=(10,3),counts=False)


plot_category_distribution(feature="Weather_conditions" , figsize=(8,3))
plot_category_counts(feature="Weather_conditions", ticks_rotation=0 , figsize=(12,3),counts=False)


plot_category_distribution(feature="Type_of_collision" , figsize=(8,3))
plot_category_counts(feature="Type_of_collision", ticks_rotation=90 , figsize=(12,3),counts=False)


plot_category_distribution(feature="Vehicle_movement" , figsize=(8,3))
plot_category_counts(feature="Vehicle_movement", ticks_rotation=90 , figsize=(12,3),counts=False)


plot_category_counts(feature="Casualty_class", ticks_rotation=0 , figsize=(12,3),counts=True)


plot_category_counts(feature="Sex_of_casualty", ticks_rotation=0 , figsize=(12,3),counts=True)


plot_category_counts(feature="Age_band_of_casualty", ticks_rotation=0 , figsize=(12,3),counts=True)


plot_category_counts(feature="Casualty_severity", ticks_rotation=0 , figsize=(12,3),counts=True)


plot_category_distribution(feature="Pedestrian_movement" , figsize=(8,3))
plot_category_counts(feature="Pedestrian_movement", ticks_rotation=0 , figsize=(12,3),counts=False)


plot_category_distribution(feature="Cause_of_accident" , figsize=(8,4))
plot_category_counts(feature="Cause_of_accident", ticks_rotation=0 , figsize=(12,3),counts=False)


def visualize_feature_distribution(df, feature_x, feature_hue=None, plot_type='count' , rotation = 0):
  """
  This function creates a crosstab and countplot visualization for the distribution of a target feature
  across different categories of another feature.

  Args:
      df (pandas.DataFrame): A DataFrame containing the data.
      feature_x (str): The name of the feature to visualize on the x-axis.
      feature_hue (str, optional): The name of the feature to use for color-coding (hue) in the plot. Defaults to None (no color coding).
      plot_type (str, optional): The type of plot to generate. Currently supports 'count' (default) and 'mean'. Defaults to 'count'.

  Returns:
      None (Displays the crosstab and plot)
  """

  # Create crosstab if no hue feature is provided
  if feature_hue is None:
    print(pd.crosstab(index=df[feature_x]))
  else:
    print(pd.crosstab(index=df[feature_x], columns=df[feature_hue]))
  print("------------------------------------------------------------------------------------------------------------------------------------------------")

  # Create plot using Seaborn FacetGrid
  grid = sns.FacetGrid(data=df, col=feature_hue if feature_hue else None, height=4, aspect=1)
  if plot_type == 'count':
    grid.map(sns.countplot, feature_x, palette=['black', 'brown', 'orange'])
  elif plot_type == 'mean':
    grid.map(sns.barplot, feature_x, 'y', palette=['black', 'brown', 'orange'])  # Assuming a numeric target variable for mean
  else:
    raise ValueError(f"Unsupported plot type: {plot_type}")
# Rotate x-axis labels to prevent overlapping
  plt.xticks(rotation=rotation)  # Adjust rotation angle as needed ,ha='right'
  plt.show()


visualize_feature_distribution(df=df, feature_x='Casualty_class', feature_hue='Accident_severity', rotation=0)


visualize_feature_distribution(df=df, feature_x='Accident_severity', feature_hue='Driving_experience', rotation=0) 


visualize_feature_distribution(df=df, feature_x='Age_band_of_casualty', feature_hue='Accident_severity') 


visualize_feature_distribution(df=df, feature_x='Hour_of_day', feature_hue='Accident_severity') 


visualize_feature_distribution(df=df, feature_x='Accident_severity', feature_hue='Educational_level',rotation=0) 





# Calculate correlation matrix
correlation_matrix = df[num_features].corr()
# Create a heatmap to visualize correlations
plt.figure(figsize=(6, 4))
sns.set()
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap='coolwarm', annot_kws={'fontsize': 8})  
# Add labels and title
plt.title('Correlation Heatmap')
plt.show()
# Identify highly correlated features (VIF calculation for additional precision)
highly_correlated = [col for col in correlation_matrix.columns if any(abs(val) > 0.8 for val in correlation_matrix[col].drop(col))]
if highly_correlated:
  print("Potentially highly correlated features:", highly_correlated)
  # Consider further analysis like VIF calculation and domain knowledge before dropping features
else:
  print("No highly correlated features identified based on correlation coefficients (further analysis might be needed with VIF).")


def calculate_vif(df):
    """
    Calculate the Variance Inflation Factor (VIF) for each explanatory variable.

    Args:
        X (pd.DataFrame): Design matrix with explanatory variables.

    Returns:
        pd.DataFrame: DataFrame with VIF values and corresponding feature names.
    """
    from statsmodels.stats.outliers_influence import variance_inflation_factor

    vif = pd.DataFrame()
    vif["VIF Factor"] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]
    vif["Features"] = df.columns
    return vif
vif_df = calculate_vif(df[num_features])
print(vif_df)
print("Values above 5 suggest moderate to severe multicollinearity, and above 10 are considered critical.")


import phik
# cat= [col for col in df.columns if col =="object"]
corr_matrix = df[[feature for feature in df.columns if df[feature].dtypes == 'object']].phik_matrix()
plot_correlation_matrix(corr_matrix.values,
                       x_labels= corr_matrix.columns,
                       y_labels= corr_matrix.index,
                       vmin=0,
                       vmax=1,
                       color_map="Purples",
                       title= "Correlation Matrix",
                       fontsize_factor= .70,
                       figsize= (12,8))


categorical_features=[feature for feature in df.columns if df[feature].dtypes == 'object']
def chi2_test(feature1, feature2, matrix):
    """
    Performs a chi-squared test on two categorical features and updates the results matrix.

    Args:
        feature1 (str): Name of the first categorical feature.
        feature2 (str): Name of the second categorical feature.
        matrix (np.ndarray): The results matrix to store chi-squared values and p-values.

    Returns:
        np.ndarray: The updated results matrix.
    """

    contingency_table = pd.crosstab(df[feature1], df[feature2])

    try:
        chi2, pval, degrees_of_freedom, expected_counts = chi2_contingency(contingency_table)
    except ValueError as e:
        print(f"Error performing chi-squared test: {e}")
        return matrix  # Return unmodified matrix on error

    # Calculate indices inside the function for clarity and avoid potential naming conflicts
    feature1_index = categorical_features.index(feature1)  # Find index of feature1
    feature2_index = categorical_features.index(feature2)  # Find index of feature2

    matrix[feature1_index][feature2_index] = chi2
    matrix[feature1_index][feature2_index + 1] = pval

    return matrix

def analyze_multicollinearity(chi2_results, corr_threshold=0.05):
    """
    Identifies potentially high-collinear feature pairs based on a p-value threshold.

    Args:
        chi2_results (pd.DataFrame): DataFrame containing chi-squared values and p-values.
        corr_threshold (float, optional): Threshold for p-value considered high. Defaults to 0.05.

    Returns:
        list: List of potentially high-collinear feature pairs.
    """
    high_corr_features = []
    for i in range(len(categorical_features)):
        for j in range(i + 1, len(categorical_features)):
            if chi2_results.iloc[i, j + 2] < corr_threshold:  # Correct p-value index
                high_corr_features.append((categorical_features[i], categorical_features[j]))

    return high_corr_features

# Analyze pairwise relationships
chi2_results = np.zeros((len(categorical_features), len(categorical_features) * 2))  # Doubled size
for i in range(len(categorical_features)):
    for j in range(i + 1, len(categorical_features)):
        feature1 = categorical_features[i]
        feature2 = categorical_features[j]
        chi2_results = chi2_test(feature1, feature2, chi2_results.copy())

# Convert matrix to DataFrame with separate columns
column_names = [f"chi2_{col}" for col in categorical_features] + [f"pval_{col}" for col in categorical_features]
chi2_results_df = pd.DataFrame(chi2_results, columns=column_names, index=categorical_features)

# Improved display formatting and handling large DataFrames
print("\nChi-Squared Test Results (Matrix):")
pd.set_option('display.max_rows', None)  # Show all rows
pd.set_option('display.max_columns', None)  # Show all columns
display(chi2_results_df)

# Analyze potential multicollinearity
high_corr_features = analyze_multicollinearity(chi2_results_df)

# Print potentially high-collinear features
if high_corr_features:
    print("\nPotentially high-collinear feature pairs:")
    for pair in high_corr_features:
        display(f"- {pair[0]} - {pair[1]}")


print(categorical_features)
print(num_features)


# Removing features with Multicollinearity or similar info
columns = ["Time"]
          # ,"Educational_level","Casualty_class","Sex_of_casualty","Age_band_of_casualty","Pedestrian_movement",
          # "Vehicle_driver_relation","Owner_of_vehicle", "Sex_of_driver", "Road_surface_type" , "Age_band_of_driver",
          # "Type_of_vehicle",]
df.drop(columns=columns , inplace=True)








display(df.head(1))
skim(df)


def remove_outliers_by_featur(df, column_name, n_std=1.5):
    mean = df[column_name].mean()
    sd = df[column_name].std()
    lower_fence = mean - (n_std * sd)
    upper_fence = mean + (n_std * sd)
    
    # Identify outliers
    outliers = df[(df[column_name] < lower_fence) | (df[column_name] > upper_fence)]
    print(f"Identified outliers: {len(outliers)}")
        # Remove outliers
    df.drop(outliers.index, inplace=True)

def remove_outliers_by_feature_class(df, column_name, class_column, n_std = 1.5):
    for class_value in df[class_column].unique():
        class_df = df[df[class_column] == class_value]
        mean = class_df[column_name].mean()
        sd = class_df[column_name].std()
        lower_fence = mean - (n_std * sd)
        upper_fence = mean + (n_std * sd)
        
        # Identify outliers for this class
        outliers = class_df[(class_df[column_name] < lower_fence) | (class_df[column_name] > upper_fence)]
        print(f"Identified outliers for class '{class_value}': {len(outliers)}")
        
        # Remove outliers for this class
        df.drop(outliers.index, inplace=True)


sns.violinplot(data=df, x="Number_of_vehicles_involved",y="Accident_severity",split=False, inner="quart",hue="Accident_severity");


sns.violinplot(data=df, x="Hour_of_day",y="Accident_severity",split=False, inner="quart",hue="Accident_severity");


# remove_outliers_by_featur(df=df, column_name="Hour_of_day", n_std=1.7)


import copy
df_cln = copy.deepcopy(df)
df_cln.shape





num_total_duplicates = len(df_cln) - len(df_cln.drop_duplicates())
print(f"Total number of duplicate rows: {num_total_duplicates}")


df_cln.sample(2)


y = df_cln["Accident_severity"]
X = df_cln.drop(columns=["Accident_severity"] , axis=1).copy()
print(X.shape, len(y))


def encode_categorical_features(df):
    """Encodes categorical features in a pandas DataFrame using label encoding.

    Args:
        df (pandas.DataFrame): The DataFrame containing the features.

    Returns:
        pandas.DataFrame: The DataFrame with encoded categorical features.
    """
    # Select categorical features
    cat_features = [feature for feature in df.columns if df[feature].dtypes == 'object']

    # Create a label encoder instance
    le = LabelEncoder()

    # Apply label encoding only to specified categorical columns
    for col in cat_features:
        df[col] = le.fit_transform(df[col])

    # Return the encoded DataFrame
    return df


# Encoding Target classes into numbers
y_encoded = replace_multiple_values(y, replacements={'Slight Injury':0, 'Serious Injury':1, 'Fatal injury':2})


X_encoded = encode_categorical_features(df=X)
X_encoded.head(2)





sns.countplot(df_cln, x="Accident_severity")
plt.show()











from imblearn.over_sampling import SMOTE , ADASYN , BorderlineSMOTE
from imblearn.combine import SMOTEENN





# Apply SMOTE to balance the target classes but it is not good for multiclass
smote = SMOTE(random_state=42 , k_neighbors=5)
X_balanced, y_balanced = smote.fit_resample(X_encoded, y_encoded)
print(X_balanced.shape, len(y_balanced))
sns.countplot(x=y_balanced);





#Assuming you have X_train and y_train
adasyn = ADASYN(random_state=42, n_neighbors=5)
X_adasyn, y_adasyn = adasyn.fit_resample(X_encoded, y_encoded)
print(X_adasyn.shape, len(y_adasyn))
sns.countplot(x=y_adasyn);





# Assuming you have X_train and y_train
borderline_smote = BorderlineSMOTE(random_state=42, k_neighbors=5)
X_border, y_border = borderline_smote.fit_resample(X_encoded, y_encoded)
print(X_border.shape, len(y_border))
sns.countplot(x=y_border);





smoteenn = SMOTEENN(random_state=42)
X_combined, y_combined = smoteenn.fit_resample(X_encoded, y_encoded)
print(X_combined.shape, len(y_combined))
sns.countplot(x=y_combined);





X_train, X_test, y_train, y_test = train_test_split(X_border, y_border, test_size=0.2, random_state=42)
X_train_balance, X_test_balance, y_train_balance, y_test_balance = train_test_split(X_balanced, y_balanced, test_size=0.2, random_state=42)
X_train_adasyn, X_test_adasyn, y_train_adasyn, y_test_adasyn = train_test_split(X_adasyn, y_adasyn, test_size=0.2, random_state=42)
X_train_border, X_test_border, y_train_border, y_test_border = train_test_split(X_border, y_border, test_size=0.2, random_state=42)
X_train_combined, X_test_combined, y_train_combined, y_test_combined = train_test_split(X_combined, y_combined, test_size=0.2, random_state=42)


# Classifiers used for Feature Selections
rf = RandomForestClassifier(n_estimators=100, random_state=42)  # Adjust n_estimators as needed
etc = ExtraTreesClassifier(n_estimators=100, random_state=42)
xgb = XGBClassifier(n_estimators=100, random_state=42)


from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS
# Multiclass Random Forest Classifier with ExhaustiveFeatureSelector >>> only use this if you have more computing power.
# Feature selector with F1-weighted scoring
sel_rf = EFS(rf, max_features=4, scoring='f1_weighted', cv=5)


# Only run this if have heavy computing power.
# model_rf = sel_rf.fit(X_train,y_train) 


# Classifier for multiclass classification ExhaustiveFeatureSelector only use this if you have more computing power.
lr = LogisticRegression(multi_class='multinomial') # If binary class you can leave empty
# Feature selector with suitable scoring for multiclass
sel_lr = EFS(lr, max_features=4, scoring='f1_weighted', cv=5)


# model_lr = sel_lr.fit(X_train,y_train) # Only run this if have heavy computing power.





sfs_rf = SequentialFeatureSelector(rf, n_features_to_select= 25, direction='backward', scoring='f1_weighted', n_jobs=-1,cv=5)
sfs_rf.fit(X_train_border,y_train_border)
# sfs_etc = SequentialFeatureSelector(etc, n_features_to_select= 25, direction='backward', scoring='f1_weighted', n_jobs=-1,cv=5)
# sfs_etc.fit(X_train_border,y_train_border)


# selected_features
selected_feature_indices = np.where(sfs_rf.support_)[0]  # Get indices of selected features
X_train_border = X_train_border.iloc[:,selected_feature_indices]
X_test_border = X_test_border.iloc[:,selected_feature_indices]
X_train_border.columns


def feature_importance_plot(df, model=None, top_n=25 , figsize=(11,7)):
  """
  Creates a bar plot visualizing feature importances from a model or DataFrame.

  Args:
      df (pd.DataFrame): DataFrame containing features and their importances (optional).
      model (object, optional): Trained model object with feature_importances_ attribute (optional).
      top_n (int, optional): Number of top features to display (default: 10).

  Returns:
      None
  """

  # Check if feature importances are provided in either df or model
  if model is not None:
    feature_importance_df = pd.DataFrame({'Feature': df.columns, 'Importance': model.feature_importances_})
  elif 'Importance' in df.columns:
    feature_importance_df = df.copy()
  else:
    raise ValueError("Feature importances not found in either provided DataFrame or model.")

  # Sort and select top features
  feature_importance_df.sort_values(by="Importance", ascending=False, inplace=True)
  top_features = feature_importance_df.head(top_n)

  # Create the bar plot with seaborn
  plt.figure(figsize=figsize)
  ax = sns.barplot(x="Importance", y="Feature", data=top_features)
  ax.set_title(f"Top {top_n} Feature Importances")
  ax.set_xlabel("Importance")
  ax.set_ylabel("Feature")    
  plt.show()


# Train a decision tree classifier
dtc = DecisionTreeClassifier()
# dtc.fit(X_train_balanced, y_train_balanced)
# dtc.fit(X_train_adasyn, y_train_adasyn)
# dtc.fit(X_train_border, y_train_border)
dtc.fit(X_train_border, y_train_border)
# Get feature importance
# dtc_feature_imp = dtc.feature_importances_
# print("Feature Importance:")
# for feature, importance in zip(X.columns, dtc_feature_imp):
#     print(f"{feature}: {importance:.4f}")
feature_importance_plot(df=X_train_border , model=dtc ,top_n=20)


# plt.figure(figsize=(10, 6))
# ax = sns.barplot(x=X.columns, y=dtc_feature_imp, orient='v')
# # Add value labels on top of each bar
# for i, v in enumerate(dtc_feature_imp):
#     ax.text(i, v, f"{v:.2f}", ha='center', va='bottom', fontweight='bold')
# plt.xlabel("Features")
# plt.ylabel("Importance")
# plt.title("Feature Importance")
# plt.xticks(rotation=90)
# plt.tight_layout()
# plt.show()


# Getting important features with RF
rfc = RandomForestClassifier()
# rfc.fit(X_train_adasyn, y_train_adasyn)
rfc.fit(X_train_border, y_train_border)
feature_importance_plot(df=X_train_border , model=rfc ,top_n=27)


# # Train a XGBFC classifier
# xgb = XGBRFClassifier()
# xgb.fit(X_train_border, y_train_border)
# # Get feature importance
# feature_importance_plot(df=X_train_border , model=xgb ,top_n=27)





def select_features_corr_based(X_train, y_train, X_test, x="all"):
    if type(x) == str:
        fs_corr = SelectKBest(score_func=f_classif, k='all')
    else:
        fs_corr = SelectKBest(score_func=f_classif, k=x)
    fs_corr.fit(X_train, y_train)
    X_train_fs = fs_corr.transform(X_train)
    X_test_fs = fs_corr.transform(X_test)

    return X_train_fs, X_test_fs, fs_corr

def select_features_infogain_based(X_train, y_train, X_test, x="all"):
    if type(x) == str:
        fs_info = SelectKBest(score_func=mutual_info_classif, k='all')
    else:
        fs_info = SelectKBest(score_func=mutual_info_classif, k=x)
    fs_info.fit(X_train, y_train)
    X_train_fs = fs_info.transform(X_train)
    X_test_fs = fs_info.transform(X_test)

    return X_train_fs, X_test_fs, fs_info


X_train_fs_corr, X_test_fs_corr, fs_corr = select_features_corr_based(X_train=X_train_border, y_train=y_train_border, X_test=X_test_border)
X_train_fs_info, X_test_fs_info, fs_info = select_features_infogain_based(X_train=X_train_border, y_train=y_train_border, X_test=X_test_border)


def fs_score_plot(fs_func):
    for i in range(len(fs_func.get_feature_names_out())):
        print('Feature %s: %f' % (fs_func.get_feature_names_out()[i], fs_func.scores_[i]))
    # plot the scores
    plt.figure(figsize=(12,5))
    plt.bar(fs_func.get_feature_names_out(), fs_func.scores_)
    plt.xticks(rotation=90)
    plt.show()

fs_score_plot(fs_corr)
fs_score_plot(fs_info)


def features_dist(column_name):
    return pd.concat([df_original[column_name].value_counts() / df_original.shape[0]*100,
        df_cln[column_name].value_counts() / df_cln.shape[0] * 100], axis=1,
        keys=[column_name + '_orginal', column_name + '_clean'])








# Create classifiers
classifiers = [
    # SVC(gamma='auto'),
    # NuSVC(gamma='auto'),
    # LinearSVC(),
    SGDClassifier(random_state=42),
    KNeighborsClassifier(),
    LogisticRegression(random_state=42),
    LogisticRegressionCV(random_state=42),
    BaggingClassifier(n_estimators=100, random_state=42),
    ExtraTreesClassifier(n_estimators=100, random_state=42),
    RandomForestClassifier(n_estimators=100, random_state=42),
    XGBClassifier(n_estimators=100, random_state=42),
    XGBRFClassifier(n_estimators=100, random_state=42)
]


# Evaluate metrics
def get_accuracy_score(y_test , y_pred):
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')
    print(f"Model Accuracy: {accuracy:.2f}")
    print(f"Model Precision: {precision:.2f}")
    print(f"Model Recall: {recall:.2f}")
    print(f"Model F1-score: {f1:.2f}")


# Create function for classifiers traning
def train_classifiers(classifiers, X_train, y_train):
    trained_classifiers = {}
    for clf in classifiers:
        clf.fit(X_train, y_train)
        trained_classifiers[clf.__class__.__name__] = clf
    return trained_classifiers


def train_classifiers_with_cv(classifiers, X, y, n_splits=5):
  """
  Trains classifiers using Stratified K-Fold Cross-Validation.

  Args:
      classifiers (list): List of classifier objects.
      X (array-like): Features data.
      y (array-like): Target labels.
      n_splits (int, optional): Number of folds for cross-validation. Defaults to 5.

  Returns:
      dict: Dictionary containing trained classifiers with their names as keys.
  """

  trained_classifiers = {}
  cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

  for clf in classifiers:
    clf_name = clf.__class__.__name__
    trained_classifiers[clf_name] = clf

    # Use Stratified K-Fold for evaluation
    for train_index, test_index in cv.split(X, y):
      # Use indexing to ensure correct data access
      X_train, X_test = X.iloc[train_index], X.iloc[test_index]
      y_train, y_test = y.iloc[train_index], y.iloc[test_index]

      # Train the classifier on the training fold
      trained_classifiers[clf_name].fit(X_train, y_train)

  return trained_classifiers


def get_scores(trained_classifiers, X_test, y_test):
    scores = {}
    for clf_name, clf in trained_classifiers.items():
        y_pred = clf.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, average="weighted")
        recall = recall_score(y_test, y_pred, average="weighted")
        f1 = f1_score(y_test, y_pred, average="weighted")
        scores[clf_name] = {
            'Accuracy': accuracy,
            'Precision': precision,
            'Recall': recall,
            'F1-score': f1
        }
        # Calculate feature importance (if available)
        # if hasattr(clf, 'feature_importances_'):
        #     importances = clf.feature_importances_
        #     indices = np.argsort(importances)
        #     num_features = 27 # Customize the number of features to display
        #     plt.figure(figsize=(12, 8))
        #     plt.title(f'Feature Importances for {clf_name}')
        #     plt.barh(range(num_features), importances[indices[-num_features:]], color='b', align='center')
        #     plt.yticks(range(num_features), X_test.columns)
        #     plt.xlabel('Importance Score')
        #     plt.show()

    return scores








sns.countplot(df_cln, x="Accident_severity");


X_train_base, X_test_base, y_train_base, y_test_base = train_test_split(X_encoded, y_encoded, test_size=0.2, random_state=42)
trained_classifiers_base = train_classifiers(classifiers=classifiers , X_train = X_train_base , y_train = y_train_base) # Option 1 with CV
# trained_classifiers_base = train_classifiers_with_cv(classifiers=classifiers , X = X_train , y = y_train) # Option 2 with CV





scores_base_models = get_scores(trained_classifiers_base, X_test_base, y_test_base)
# Visualize results with Base Models 27 Features
df_scores_base_models = pd.DataFrame(scores_base_models)
df_scores_base_models





print(X_balanced.shape, len(y_balanced))
sns.countplot(x=y_balanced);


trained_classifiers_base = train_classifiers(classifiers=classifiers , X_train = X_train , y_train = y_train) # Option 1 with CV
# trained_classifiers_base = train_classifiers_with_cv(classifiers=classifiers , X = X_train , y = y_train) # Option 2 with CV





scores_base_models = get_scores(trained_classifiers_base, X_test, y_test)
# Visualize results with Base Models 27 Features
df_scores_base_models = pd.DataFrame(scores_base_models)
df_scores_base_models


feature_importance_plot(df=X_train , model=trained_classifiers_base["ExtraTreesClassifier"] ,top_n=20)


feature_importance_plot(df=X_train , model=trained_classifiers_base["RandomForestClassifier"] ,top_n=20)





trained_classifiers_bf = train_classifiers(classifiers=classifiers , X_train = X_train_border , y_train = y_train_border)
# trained_classifiers_bf = train_classifiers_with_cv(classifiers=classifiers , X = X_train_border , y = y_train_border)





scores_bf = get_scores(trained_classifiers_bf, X_test_border, y_test_border)
# Visualize results with Best 25 Features 
df_scores_bf = pd.DataFrame(scores_bf)
df_scores_bf


feature_importance_plot(df=X_train_border , model=trained_classifiers_bf["ExtraTreesClassifier"] ,top_n=20)


feature_importance_plot(df=X_train_border , model=trained_classifiers_bf["RandomForestClassifier"] ,top_n=20)


plt.figure(figsize=(12, 5))
sns.barplot(data=df_scores_bf, palette='viridis')
plt.title('Classifier Performance Metrics')
plt.ylabel('Score')
plt.ylim(0, 1)
plt.xticks(rotation=90)
plt.show()








# Define the parameter grid for GridSearchCV
param_grid = {
    'n_estimators': [50, 80, 100, 200, 250],  # Number of trees in the forest
    "criterion": ["gini", "entropy"],
    "max_depth": [5, 10, 15, 20, 25, 30],
    'max_features': ['sqrt', 'log2'], # Maximum features considered for each split or "max_features": [3, 5]
    'min_samples_split': [2, 3, 4],  # Minimum samples required to split an internal node
    'min_samples_leaf': [1, 2], # Minimum number of samples required at each leaf node.
    'bootstrap': [True, False]  # Whether bootstrap samples are used
}
# Create the Extra Tree Classifier
etc_model = ExtraTreesClassifier(random_state=42)


# Initialize K-Fold cross-validation
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
# Initialize GridSearchCV
grid_search = GridSearchCV(estimator=etc_model, param_grid=param_grid, 
                           cv=skf, scoring='f1_weighted',n_jobs=-1)


# Fit the model to the training data 
# grid_search.fit(X_train_border, y_train_border)


# Get the best parameters and best score
# best_params = grid_search.best_params_
# best_score = grid_search.best_score_

# print("Best Score:", best_score)
# display("Best Parameters:", best_params)


# Creating a new Extra Tree model using the best parameters obtained by GridSearchCV
best_params = {'bootstrap': False, 'criterion': 'gini', 'max_depth': 30,'max_features': 'log2','min_samples_leaf': 1,
               'min_samples_split': 2,'n_estimators': 250}
best_etc_model = ExtraTreesClassifier(random_state=42 ,**best_params)
best_etc_model.fit(X_train_border, y_train_border)
# Predict on the test set
y_pred = best_etc_model.predict(X_test_border)
get_accuracy_score(y_test_border, y_pred)





import optuna
from optuna.integration import sklearn





class StopWhenTrialKeepBeingPrunedCallback:
    def __init__(self, threshold: int):
        self.threshold = threshold
        self._consequtive_pruned_count = 0

    def __call__(self, study: optuna.study.Study, trial: optuna.trial.FrozenTrial) -> None:
        if trial.state == optuna.trial.TrialState.PRUNED:
            self._consequtive_pruned_count += 1
        else:
            self._consequtive_pruned_count = 0

        if self._consequtive_pruned_count >= self.threshold:
            study.stop()
study_stop_cb = StopWhenTrialKeepBeingPrunedCallback(2)


# Define your objective function (replace with your actual evaluation metric)
def objective(trial):
  # Suggest hyperparameters
  n_estimators = trial.suggest_int("n_estimators", 100, 1000)
  max_depth = trial.suggest_int("max_depth", 10, 50)
  min_samples_split = trial.suggest_int("min_samples_split", 2, 20)
  min_samples_leaf = trial.suggest_int("min_samples_leaf", 1, 5)

  # Create and train the model with suggested hyperparameters
  model = ExtraTreesClassifier(n_estimators=n_estimators, max_depth=max_depth,
                                min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf)
  model.fit(X_train, y_train)

  # Evaluate the model on the validation set (replace with your validation logic)
  y_pred = model.predict(X_val)
  accuracy = accuracy_score(y_val, y_pred)

  # Return the negative accuracy (Optuna minimizes the objective)
  return -accuracy

# Load your data (X: features, y: target labels)
X_train, X_val, y_train, y_val = train_test_split(X_border, y_border, test_size=0.2)

# Create the Optuna study
study = optuna.create_study(direction="minimize")

# Optimize the objective function for a specified number of trials
study.optimize(objective, n_trials=20 , callbacks=[study_stop_cb])

# Get the best hyperparameters
best_params = study.best_params

# Print the best hyperparameters
print("Best hyperparameters:", best_params)

# Train the final model with the best hyperparameters
best_model = ExtraTreesClassifier(**best_params)
best_model.fit(X_train, y_train)





# Creating a new Extra Tree model using the best parameters obtained by Optuna
best_params = {'bootstrap': False, 'criterion': 'gini', 'max_depth': 26,'max_features': 'log2','min_samples_leaf': 1,
               'min_samples_split': 2,'n_estimators': 691}
best_etc_model = ExtraTreesClassifier(random_state=42 ,**best_params)
best_etc_model.fit(X_train_border, y_train_border)
# Predict on the test set
y_pred = best_etc_model.predict(X_test_border)








y_pred = best_model.predict(X_val)
get_accuracy_score(y_val, y_pred)


feature_importance_plot(df=X_train , model=best_model ,top_n=20)


from sklearn.metrics import confusion_matrix

# Assuming you have your true labels (y_true) and predicted labels (y_pred)

# Create the confusion matrix
cm = confusion_matrix(y_test_border, y_pred)

# Print the confusion matrix (optional)
print(cm)

# Visualization with two-digit formatting
import seaborn as sns
import matplotlib.pyplot as plt

# Create a heatmap with integer formatting
ax = sns.heatmap(cm, annot=True, cmap="Blues", fmt="d")  # Use 'd' for integer formatting

# Customize plot elements (optional)
ax.set_title("Confusion Matrix")
ax.set_xlabel("Predicted Label")
ax.set_ylabel("True Label");




















import lime
from lime import lime_tabular  # Import the specific function


# Create LIME explainer (adjust for your model type and feature names)
# explainer = lime_tabular(X_train, X_train.columns, mode='classification')  # Adjust mode if needed

# # Explain the instance
# explanation = explainer.explain_instance(instance, class_names=range(best_model.n_classes_))

# # Print LIME explanation (adjust output format if needed)
# print("LIME Explanation:")
# print(explanation.as_text())  # Consider using explanation.as_pyplot_fig() for visualization









import deeplift as dl
# Load your trained model (replace with your actual model)
model = best_model

# Load a single data instance for explanation (adjust indexing for your data)
instance = X_train.iloc[0].values.reshape(1, -1)  # Reshape for single instance

# Target class for explanation (if applicable)
target_class = None  # Adjust if you want to explain prediction for a specific class


# Define DeepLIFT explainer (replace with your specific model architecture)
def deeplift_explainer(predicted_class):
  # DeepLIFT explainer logic specific to your model architecture
  return dl.Rescale(dl.GradientShap(model, reference_input=reference_input))  # Adjust as needed

# Load reference input (optional, consider using mean or median of training data)
reference_input = np.random.rand(1, instance.shape[1])  # Replace with your reference input strategy

# Get DeepLIFT explainer for the target class (if applicable)
deeplift_exp = deeplift_explainer(target_class)

# Calculate DeepLIFT scores
scores = deeplift_exp.explain(instance)

# Print DeepLIFT explanation (adjust output format if needed)
print("\nDeepLIFT Explanation:")
# Print individual feature scores or a summary
print(f"Feature Scores: {scores}")  # Or consider printing a more informative summary









